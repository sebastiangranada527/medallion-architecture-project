{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6cb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "4.0.1\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark -q\n",
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105b6458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/22 21:18:11 WARN Utils: Your hostname, Mac-mini.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.1 instead (on interface en1)\n",
      "25/12/22 21:18:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/22 21:18:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Iniciamos Spark en modo local\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Medallion - Capa Bronze\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1395e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_date: date (nullable = true)\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- total: double (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n",
      "+----------+--------+------------------------------------+----------+--------+-------+-------+---------+\n",
      "|sale_date |store_id|customer_id                         |product   |quantity|price  |total  |status   |\n",
      "+----------+--------+------------------------------------+----------+--------+-------+-------+---------+\n",
      "|2025-07-27|NW-166  |8b1ff910-f298-4568-8f85-d7879a818f90|Smartphone|2       |1190.42|2380.84|Completed|\n",
      "|2025-01-15|NW017   |03afe752-6931-46bf-a7f2-f1d57ddba376|Headphones|1       |1026.98|1026.98|Completed|\n",
      "|2025-03-12|NW055   |7d3928fa-7308-4afb-9c1f-0759e812d758|Monitor   |1       |NULL   |575.75 |Returned |\n",
      "|2025-03-14|NW-142  |9670544b-48ea-4a5d-946b-1798f7d40349|Smartphone|5       |1084.0 |5420.0 |Completed|\n",
      "|2025-09-16|078-NW  |7a87eccb-7518-413b-934e-29ee2534a622|Laptop    |3       |647.28 |1941.84|Returned |\n",
      "+----------+--------+------------------------------------+----------+--------+-------+-------+---------+\n",
      "only showing top 5 rows\n",
      "Datos guardados en capa Bronze: ../data/bronze/sales_raw.parquet\n"
     ]
    }
   ],
   "source": [
    "# Ruta al CSV generado en Etapa 1\n",
    "input_path = \"../../etapa1-fundamentos/data/raw_sales.csv\"\n",
    "output_path = \"../data/bronze/sales_raw.parquet\"\n",
    "\n",
    "# Leemos el CSV crudo (sin ninguna transformación)\n",
    "df_bronze = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "# Mostramos esquema y algunas filas para verificar\n",
    "df_bronze.printSchema()\n",
    "df_bronze.show(5, truncate=False)\n",
    "\n",
    "# Guardamos en formato Parquet (eficiente y columnar)\n",
    "df_bronze.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Datos guardados en capa Bronze: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977fd114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas en CSV original: 5000\n",
      "Filas en Parquet Bronze: 5000\n",
      "+----------+--------+--------------------+----------+--------+-----+-------+---------+\n",
      "| sale_date|store_id|         customer_id|   product|quantity|price|  total|   status|\n",
      "+----------+--------+--------------------+----------+--------+-----+-------+---------+\n",
      "|2025-03-12|   NW055|7d3928fa-7308-4af...|   Monitor|       1| NULL| 575.75| Returned|\n",
      "|2025-06-16|  011-NW|f0f8ec8b-3553-498...|   Monitor|       2| NULL|1506.66|Completed|\n",
      "|2025-10-05|  NW-122|d4b325bb-7cdf-441...|    Tablet|       4| NULL|3618.72|Completed|\n",
      "|2025-10-18|   NW-59|32515277-cdc5-452...|Smartphone|       4| NULL|3921.12|Completed|\n",
      "|2025-09-02|   NW062|3cd230cb-10f4-491...|    Tablet|       4| NULL|4605.52|Completed|\n",
      "+----------+--------+--------------------+----------+--------+-----+-------+---------+\n",
      "only showing top 5 rows\n",
      "+--------+\n",
      "|store_id|\n",
      "+--------+\n",
      "|NW-156  |\n",
      "|NW-49   |\n",
      "|NW095   |\n",
      "|NW-175  |\n",
      "|152-NW  |\n",
      "|058-NW  |\n",
      "|NW-60   |\n",
      "|NW010   |\n",
      "|NW-33   |\n",
      "|060-NW  |\n",
      "|NW027   |\n",
      "|NW-157  |\n",
      "|015-NW  |\n",
      "|NW-77   |\n",
      "|NW-169  |\n",
      "|NW-128  |\n",
      "|NW081   |\n",
      "|NW106   |\n",
      "|NW-134  |\n",
      "|177-NW  |\n",
      "+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Leemos de vuelta para confirmar que todo está intacto\n",
    "df_bronze_loaded = spark.read.parquet(output_path)\n",
    "\n",
    "# Conteo de filas (debe coincidir con el CSV original)\n",
    "print(\"Filas en CSV original:\", df_bronze.count())\n",
    "print(\"Filas en Parquet Bronze:\", df_bronze_loaded.count())\n",
    "\n",
    "# Verificamos que los nulos y formatos raros de store_id siguen intactos\n",
    "df_bronze_loaded.filter(df_bronze_loaded[\"price\"].isNull()).show(5)\n",
    "df_bronze_loaded.select(\"store_id\").distinct().show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab817a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos tardíos agregados a Bronze. Total filas ahora: 5500\n"
     ]
    }
   ],
   "source": [
    "# Generamos un pequeño batch adicional (simulando datos que llegan después)\n",
    "from faker import Faker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fake = Faker()\n",
    "np.random.seed(42)\n",
    "\n",
    "late_data = []\n",
    "for _ in range(500):\n",
    "    sale_date = fake.date_between(start_date=\"-30d\", end_date=\"today\")\n",
    "    store_id = f\"NW-{np.random.randint(1, 181):03d}\"\n",
    "    customer_id = fake.uuid4()\n",
    "    product = \"Extra Batch Product\"\n",
    "    quantity = np.random.randint(1, 6)\n",
    "    price = round(np.random.uniform(100, 500), 2)\n",
    "    total = quantity * price\n",
    "    status = \"Completed\"\n",
    "\n",
    "    late_data.append([sale_date, store_id, customer_id, product, quantity, price, total, status])\n",
    "\n",
    "df_late = pd.DataFrame(late_data, columns=[\"sale_date\", \"store_id\", \"customer_id\", \"product\", \"quantity\", \"price\", \"total\", \"status\"])\n",
    "\n",
    "# Guardamos temporalmente\n",
    "late_csv = \"../data/bronze/late_sales.csv\"\n",
    "df_late.to_csv(late_csv, index=False)\n",
    "\n",
    "# Lo ingirimos en modo append a Bronze\n",
    "df_late_spark = spark.read.csv(late_csv, header=True, inferSchema=True)\n",
    "df_late_spark.write.mode(\"append\").parquet(output_path)\n",
    "\n",
    "print(\"Datos tardíos agregados a Bronze. Total filas ahora:\", spark.read.parquet(output_path).count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
